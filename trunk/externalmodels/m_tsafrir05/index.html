<html>
<head>
<title>Parallel Workloads Archive: 
       A Model to Generate User Runtime Estimates</title>
<base href="http://www.cs.huji.ac.il/labs/parallel/workload/m_tsafrir05/">
<link rel=stylesheet type="text/css" 
      href="http://www.cs.huji.ac.il/labs/parallel/workload/wl.css">
<link rel="shortcut icon" 
      href="http://www.cs.huji.ac.il/labs/parallel/workload/ico.gif">
<meta name="author" content="Dan Tsafir">
</head>

<body>
<h1>A Model to Generate User Runtime Estimates and Append Them to an
SWF File
</h1>

A C++ program and API to artificially generate a distribution of user
runtime estimates, and optionally append the generated estimates to an
existing 
<a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
Standard Workload Format (SWF)</a> file (which in turn can be either
a real log or generated by some other model).

This model is the result of the paper
<a href="Est05JSSPP.pdf">Modeling User Runtime Estimates</a>
published in JSSPP'05
[<a
href="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#tsafrir05b"
>tsafrir05b</a>].


<!----------------------------------------------------------------------------->
<p>
<table border="1" style="text-align: left; background:lavender">

<tr>
        <td colspan="3"><center><b>Quick how-to:</b></center></td>
        </tr>
<tr>
        <td>#</td>
	<td><center><i>what</i></center></td>
	<td><center><i>how</i></center></td>
        </tr>

<tr>
        <td>1</td>
	<td>download this tar file</td>
	<td><a href="est.tgz">est.tgz</a></td>
        </tr>
<tr>
        <td>2</td>
        <td>open the tar and compile code</td>
        <td><tt>tar -xvzf est.tgz<br>
	        cd est<br>
		make</tt></td>
        </tr>
<tr>
        <td>3</td>
        <td>run the program and follow the instructions</td>
        <td><tt>./est_driver</tt></td>
        </tr>
</table>
</p>
<br>
<!----------------------------------------------------------------------------->


<!----------------------------------------------------------------------------->
<p><br>
<img src="line.gif">
<a name="toc"><h3>Table of Content:</h3></a>
<!----------------------------------------------------------------------------->
<ul>
<li><a href="index.html#intro"     >Introduction       </a></li>
<li><a href="index.html#download"  >Files to download  </a></li>
<li><a href="index.html#usage"     >Usage              </a></li>
<li><a href="index.html#api"       >API                </a></li>
<li><a href="index.html#params"    >Choosing main parameters</a></li>
<li><a href="index.html#lublin" >
       Example: adding estimates</a> to the
       <a href=
       "http://www.cs.huji.ac.il/labs/parallel/workload/models.html#lublin99">
       Lublin model</a>
<li><a href="index.html#osc" >
       Example: adding estimates</a> to the
       <a href=
       "http://www.cs.huji.ac.il/labs/parallel/workload/l_osc.html">
       OSC log</a>
</a></li>
<li><a href="index.html#bugs"    >Bugs</a></li>
</ul>


<!----------------------------------------------------------------------------->
<p><br>
<img src="line.gif">
<a name="intro"><h3>Introduction:</h3></a>
<!----------------------------------------------------------------------------->

Runtime estimates provided by users to schedulers of parallel machines
have the unfortunate property of worsening performance.
Therefore, in order to get realistic performance evaluation results,
one must use realistic estimates, as provided by the model suggested
here.
All the details that explain why user estimates are so bad for
performances along with justifying the model suggested here are found
in the paper that accompanies this model titled
"<a href="Est05JSSPP.pdf">Modeling User Runtime Estimates</a>"
[<a
href="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#tsafrir05b"
>tsafrir05b</a>].
This document provides a brief overview about the available files, how
to use the model, and how to choose its parameters.





<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="download"><h3>Files to download:</h3></a>
<!----------------------------------------------------------------------------->
<p>
The file <a href="est.tgz">est.tgz</a> contains all the files
that compose the estimate-model utility. These are:
<p>

<table border="1" style="background:lavender">
<tr>
   <td><b><center><i>#</b></i></center></td>
   <td><b><center><i>file</i></center></b></td>
   <td><b><center><i>description</i></center></b></td>
   </tr>
<tr>
   <td>1</td>
   <td><a href="est_model.hh">est_model.hh</a>,
       <a href="est_model.cc">est_model.cc</a></td>
   <td>The API of the model (only 2 functions; described below) and
       its implementation.</td>
   </tr>

<tr>
   <td>2</td>
   <td><a href="est_swf_job.hh">est_swf_job.hh</a></td>
   <td>Defines the <tt>Job_t</tt> structure that encapsulates a single 
       <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
       SWF</a>-file record, and knows how to read/write it.</td>
   </tr>

<tr>
   <td>3</td>
   <td><a href="est_driver.cc">est_driver.cc</a></td>
   <td>A simple shell utility to generate an estimate distribution and
       optionally combines it with an existing
       <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
       SWF</a>-file.
       Contains the <tt>main</tt> function of the utility
       and can be viewed as an example of how to use the model's API.</td>
   </tr>

<tr>
   <td>4</td>
   <td><a href="est_parse.hh">est_parse.hh</a>,
       <a href="est_parse.cc">est_parse.cc</a></td>
   <td>Used by the shell utility to parse its command line arguments
       and optionally an 
       <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
       SWF</a>-file.</td>
   </tr>

<tr>
   <td>5</td>
   <td><a href="Makefile">Makefile</a></td>
   <td>Compiles the above and generates the <tt>est_driver</tt> utility.</td>
   </tr>

 <td>6</td>
   <td><a href="Est05JSSPP.pdf">Est05JSSPP.pdf</a></td>
   <td>The paper that explains the rationale and defines this model.</td>
   </tr>

<td>7</td>
   <td><a href="index.html">index.html</a></td>
   <td>This file.</td>
   </tr>

</table>




<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="usage"><h3>Usage:</h3></a>
<!----------------------------------------------------------------------------->
<p>
<i>SYNOPSIS</i>
<pre>
    est_driver &lt;maxest> &lt;njobs|swf> [-b&lt;userbins>] [-p&lt;precision>] [-s&lt;seed>]
</pre>

<!---------------------------------------------------->
<p>
<br>
<i>PARAMETERS</i>
<p>
<table border="1" style="background:lavender">

<tr>
  <td><center><b><i>parameter</i></b></center></td>
  <td><center><b><i>description</i></b></center></td>
  </tr>

<tr>
  <td><tt>maxest</tt><br><i>[mandatory]</i></td>
  <td>The maximal allowed estimate in seconds. If appending estimates
      to an <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a>-file, this is also the maximal allowed runtime (as job are
      killed if they exceed their estimate) and therefore all runtimes
      that are bigger than <tt>maxest</tt> will be truncated to be exactly
      <tt>maxest</tt>.
      <br>
      On how exactly to choose this value, see below.
      </td>
<tr>
  <td><tt>njobs</tt> | <tt>swf</tt><br><i>[mandatory]</i></td>
  <td>The second parameter is either <tt>njobs</tt> or <tt>swf</tt>:

      <p>

      <tt>njobs</tt>: means "number of jobs". 
      This must be a positive integer that indicates the number of
      estimates that the model must produce.
      If <tt>njobs</tt> is used then the resulting output would be the 
      estimates distribution (in a PDF and CDF form).

      <p>

      <tt>swf</tt>: this is a name of an
      <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a>-file.
      If <tt>swf</tt> is given then 
      (1) <tt>njobs</tt> is taken to be the number of jobs within the 
          <tt>swf</tt>, 
      (2) the <tt>swf</tt> is parsed, 
      (3) artificial estimates are added to it, and
      (4) the <tt>swf</tt> is reprinted with the new estimates.
      </td>
      </tr>

<tr>
  
  <td><a name="userbins"></a><tt>userbins</tt><br><i>[optional]</i><br></td>
  <td>A user-specified list of estimate bins composed of
      comma-separated pairs of the form <i>seconds=percent</i> e.g.
      <i>3600=10.5,300=2.2</i> means 10.5% of the jobs will be set to
      use 1 hour as estimate (3600 sec) and 2.2% of the jobs will
      be set to use 5 minutes estimate (300 seconds).
      <p>
      <font size="-1">
      Though not mandatory, <u>this option is especially important</u>
      because it allows the user to associate <tt>maxest</tt> with the
      amount of jobs that use it. This is important because:
      <ol>
      <li>The number of jobs that are associated with <tt>maxest</tt>
          is always substantial, but varies significantly between log to
	  log (unlike other aspects of user estimates that are almost
          identical).
	  </li>
      <li>The amount of jobs associated with <tt>maxest</tt> has
          decisive effect on performance: the more jobs associated with
	  <tt>maxest</tt>, the worst the performance of the scheduler
          becomes (as jobs are indistinguishable in the eyes of the
	  scheduler and cannot be backfilled).  
	  </li>
      </ol>
      For further discussion
      about if and how to choose <tt>userbins</tt>, see below.
      </font>
      </td>
      </tr>

<tr>
  <td><tt>precision</tt><br><i>[optional]</i><br>
  <td>In case an <tt>swf</tt> file is given (which means it is
      reprinted with artificial estimates), this parameter (a
      nonnegative integer) determines the number of precision digits
      in the output, that is, the number of decimal digits to the
      right of the floating point, in all the 
      <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a> fields that have a floating-point nature (e.g. runtime,
      used-memory, etc).
      The default is 0.
      </td>
  </tr>

<tr>
  <td><tt>seed</tt><br><i>[optional]</i><br>
  <td>The seed for the random number generator used (using the drand48
      family). Default is 0.</td>
      </tr>

</table>

<!---------------------------------------------------->
<p>
<br>
<i>EXAMPLES</i>
<p>
<table border="1" style="background:lavender">

<tr>
  <td><center><b><i>#</i></b></center></td>
  <td><center><b><i>example</i></b></center></td>
  </tr>


<tr>
<td>1</td>
<td><pre>est_driver 64800 <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_ctc_sp2.html">l_ctc_sp2.swf</a></pre>
    Reads the 
    <a
    href="http://www.cs.huji.ac.il/labs/parallel/workload/l_ctc_sp2.html">
    CTC-SP2</a> log, replaces the original user estimates with
    artificial estimates generated by the model (using its default
    settings) and prints the  resulting 
    <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
    SWF</a> file.
    The maximal value of generated estimates is set to be 18 hours
    (64800 seconds). Indeed this was administrative maximal value of
    runtimes and estimates in the 
    <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_ctc_sp2.html">
    CTC-SP2</a> site.
    <p>
    The maximal allowed value relates both for runtimes and for
    estimates, as users are not allowed to provide estimates that are
    bigger than the maximal allowed runtime.
    However, it is almost always the case that a few jobs have
    runtimes that are bigger than the maximal value allowed.
    Usually, the difference between the actual runtime and the maximal
    value is less than one minute, and probably reflects the time it
    takes to kill a job that exceeded its runtime.
    But on very rare occasions, the difference is much bigger
    (probably because the system administrator explicitly allowed
    these jobs to continue running beyond their runtime estimates).
    <p>
    Regardless of the reason, if a job's runtime is bigger than the
    maximal allowed value (=<tt>maxest</tt>, the current model
    truncates the runtime of the job to be exactly <tt>maxest</tt>).
    </td>
    </tr>

<tr>
<td>2</td>
<td><pre>est_driver 64800 <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_ctc_sp2.html">l_ctc_sp2.swf</a> -b 64800=23.8 -s 2</pre>
    Same as in th previous example, but now 23.8% of the jobs are
    assigned with an estimate of 18 hours (64800 seconds). 
    Indeed, in the original
    <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_ctc_sp2.html">
    CTC-SP2</a> trace file, 23.8% of the jobs are associated with a
    user estimates of 18h.
    The default settings of this model (used in the previous example)
    associates around 22% of the jobs with the maximal estimate.
    The seed for the random number generator used by the model is set to be 2.
    A different seed will results in a different artificial
    distribution of user runtime estimates.
    </td>
    </tr>

<tr>
<td>3</td>
<td><pre>est_driver 129600  <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_sdsc_blue.html">l_sdsc_blue.swf</a> -b 129600=1.1,7200=27.3</pre>

    This is similar to the above example: here we use a maximal
    estimate value of 36h (129,600 seconds) for the 
    <a
    href="http://www.cs.huji.ac.il/labs/parallel/workload/l_sdsc_blue.html">
    SDSC-BLUE</a> log and instruct the model to associate only
    1.1% of the jobs with this value, while associating 27.3% of the
    jobs with a smaller estimate of 2h (7200 seconds).
    <p>
    Indeed, the  <a
    href="http://www.cs.huji.ac.il/labs/parallel/workload/l_sdsc_blue.html">
    SDSC-BLUE</a> log is somewhat different than usual: the number of
    jobs associated with the maximal estimate is very small.
    <p>
    The reason for this abnormality is that the "real" (or "de-facto")
    maximal estimate of this log is 2h, as this is the runtime upper bound
    of jobs submitted to the "express" and "interactive" queues of the 
    the <a
    href="http://www.cs.huji.ac.il/labs/parallel/workload/l_sdsc_blue.html">
    SDSC BLUE Horizon</a> machine, that are used by most jobs.
    <p>
    Note that in order to change the "effective" maximal estimate of a
    log from <tt>maxest</tt> to a smaller value (let this value be
    denoted <i>E</i>), it is not enough to instruct the model to
    associate a lot of jobs with <i>E</i> (e.g. in the above example
    <tt>"-b 7200=27.3"</tt> is insufficient) because, unless
    instructed otherwise, the model will always associate very many
    jobs with <tt>maxest</tt>. Therefore, we must explicitly indicate
    that we want the effective maximal value to be changed by
    associating this value with a small number of jobs.
    Indeed, in the  <a
    href="http://www.cs.huji.ac.il/labs/parallel/workload/l_sdsc_blue.html">
    SDSC-BLUE</a> log, only 1.1% of the jobs are associated with the
    maximal value, whereas 27.3% of the jobs are associated with
    the maximal value of the two high priority queues, which is
    <i>E=2h</i>.
    </td>
    </tr>
</table>


<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="api"><h3>API:</h3></a>

The above describes how to generate an estimate distribution using the
shell utility <tt>est_driver</tt>.  However, it also very easy (and
provides more control over model parameters) to use this model as a
C++ module and invoke its functions directly.
The model's interface is composed of only two functions that use three
structure types, all defined in <a
href="est_model.hh">est_model.hh</a> (with the exception of the first
structure).  
Here is the specification of the interface:



<p>
<!----------------------------------------------------------------------------->
<table border="1" style="background:lavender">

<tr>
  <td><center><b><i>#</i></b></center></td>
  <td><center><b><i>structure / function</i></b></center></td>
  <td><center><b><i>usage</i></b></center></td>
  </tr>

<!----------------------------->
<tr>
  <td>1</td>
  <td><pre>
struct Job_t {
    int id;
    int submit;
    int wait;
    int runtime;
    <font size="-1">// all the res of the SWF fields...</font>
};

typedef std::vector&lt;Job_t> SWF_t;
</pre>
  </td>
  <td>The <tt>Job_t</tt> structure encapsulates all 18 data fields of
      one job (=line =record) within an 
      <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a>-file.
      This structure is defined in 
      <a href="est_swf_job.hh">est_swf_job.hh</a>.
      <p>
      When assigning user estimates to an existing 
      <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a>-file, 
      the latter is represented using a standard vector of
      <tt>Job_t</tt>-s.
      (We denote this vector type as <tt>SWF_t</tt>, for the
      purpose of explaining the interface here; but the type  
      <tt>std::vector&lt;Job_t></tt> is directly used within 
      <a href="est_model.hh">est_model.hh</a>.)

      <p>
      <font size="-1">
      Though not part of the interface, you can use the
      very short function in <a href="est_parse.cc">est_parse.cc</a>
      called <tt>parse_swf()</tt>, that given a name of an
       <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a>-file, easily and efficiently parses the file and
      generates the associated <tt>SWF_t</tt> job vector.
      </font>

      </td>
  </tr>



<!----------------------------->
<tr>
  <td>2</td>
  <td><pre>
struct EstBin_t {
    int time;  <font size="-1">// estimate in seconds</font>
    int njobs; <font size="-1">// # of jobs using it</font>
};

typedef std::vector&lt;EstBin_t> Dist_t;
</pre>
  </td>
  <td>This structure represents one "bin" in the estimate histogram:
      the <tt>time</tt> field holds the estimate value (in seconds) and
      the <tt>njobs</tt> field specifies the number of jobs that have
      <tt>time</tt> as their user estimate.
      <p>
      To represent the entire estimate distribution we use a standard
      vector of <tt>EstBin_t</tt>-s.
      (We denote this vector type as <tt>Dist_t</tt>, for the
      purpose of explaining the interface here; but the type  
      <tt>std::vector&lt;EstBin_t></tt> is directly used within 
      <a href="est_model.hh">est_model.hh</a>.)
      </td>
  </tr>



<!----------------------------->
<tr>
  <td>3</td>
  <td><pre>
struct EstParams_t {

    <font size="-1">// model parameters</font>
    ...

    // constructor
    EstParams_t(
        int    njobs,
        int    maxest,
	Dist_t userbins=Dist_t() );
};

</pre></td>
  <td>The are more than a dozen parameters for the estimate model
      (encapsulated within the <tt>EstParams_t</tt> structure).
      However, the constructor of <tt>EstParams_t</tt> accepts only
      the most important ones (2 mandatory, one optional):
      <p>
      <table border="1">
      <tr>
          <td><tt>njobs</tt></td>
	  <td>The number of job there are (which is the number of
              estimates the model must produce).
	      </td>
          </tr>
      <tr>
          <td><tt>maxest</tt></td>
	  <td>The maximal allowed estimate, as define above.</td>
          </tr> 
      <tr>
          <td><tt>userbins</tt></td>
	  <td>Allows to determine (e.g.) the important (and highly
	      varying) number of jobs associated with <tt>maxest</tt> 
	      (the importance of this data is explained
	      <a href="index.html#userbins">above</a>).
	      All user supplied estimate bins can be specified within the
	      <tt>userbins</tt> vector.
	      </td>
	  </tr>
      </table>
      </p>
      The other parameters are briefly described in 
      <a href="est_model.hh">est_model.hh</a> and explained in detail
      in [<a href
      ="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#tsafrir05b"
      >tsafrir05b</a>].
      All these parameters are supplied with reasonable default values
      (as detemined by [<a href
      ="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#tsafrir05b"
      >tsafrir05b</a>]),
      in the constructor of <tt>EstParams_t</tt>.
      However, you can change them by explicitly assigning other values
      to the fields of <tt>EstParams_t</tt>.
      </td>
  </tr>


<!----------------------------->
<tr>
  <td>4</td>
  <td><pre>
void 
est_gen_dist(EstParams_t params, 
             Dist_t*   est_dist);

</pre>
  </td>
  <td>Given <tt>params</tt> (specifies the model parameters), assign
      the generated estimate distribution into the place pointed to by
      <tt>est_dist</tt> (a pointer to some preallocated vector).
      </td>
  </tr>


<!----------------------------->
<tr>
  <td>5</td>
  <td><pre>
void 
est_assign(const Dist_t& est_dist,
           SWF_t*            jobs);

</pre>
  </td>
  <td>Given <tt>est_dist</tt> (an estimate distribution generated by
      the function <tt>est_gen_dist()</tt>) and <tt>jobs</tt> (a
      pointer to a vector that holds the content of some 
      <a href="http://www.cs.huji.ac.il/labs/parallel/workload/swf.html">
      SWF</a>-file with <tt>njobs</tt> records), randomly assign the
      artificial estimates as embodied by <tt>est_dist</tt> into the
      <tt>jobs</tt>, such that the estimate of each job is equal to or
      bigger than the runtime of that job.
      </td>
  </tr>




</table>
</p>


<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="params"><h3>Choosing main parameters</h3></a>


<p>
<i>BACKGROUND</i>
</p>

<!----------------------------------------->
<ul>

<!--------->
<li><p>

In order to correctly choose the main parameters of the model, you
must be aware of two simple facts:
  <ol>
  <li>The estimate distribution is very <i>modal</i>, e.g. human users
      will always extensively use, say, 1 hour as an estimate. 
      Indeed, the number of popular estimate values is very small and
      it turns out ~90% of the jobs use the same 20 estimate
      values.<p></li>

  <li>Usually, the most popular value is <tt>maxest</tt> -- the
      maximal allowed estimate.  <tt>maxest</tt> also serves as the
      upper bound of job runtimes, as a job is killed if it exceeds
      its estimate.</li>
  </ol>

</p></li>

<!--------->
<li><p>
This is exemplified in the following figure that shows the cumulative
distribution function (CDF) of runtimes and estimates, in 4 different
production systems.

<p>
<img src="cdfs/cdfs.gif" border="1">
</p>

The Y-axis shows the percent of jobs that are associated with a
value that is equal to or smaller than the corresponding X-value.
For example, in KTH, ~30% of the jobs have runtime <= 1m (one minute)
and ~6% of the jobs have 1m estimate.
The estimate curves look like a staircase because of the modal nature
of human approximations.
The larger the stair is, the more popular the associated estimate,
e.g., in SDSC-SP2 ~10% of the jobs use estimate=5m; in SDSC-BLUE ~27%
of the job use estimate=2h.
Note that the fact estimate-curves are lower than runtime-curves
indicates runtimes are shorter than estimates (which is only natural
as jobs are killed once their estimate is reached).

</p></li>

<!--------->
<li><p>
Recall <tt>maxest</tt> is usually the most popular estimate. 
This is also true for KTH and BLUE, despite appearances.
In KTH, <tt>maxest</tt> is actually 4h during weekdays (and 15h during
weeknights, and <tt>60h</tt> during weekends).
In BLUE, as mentioned earlier, the <tt>maxest</tt> of the "express"
and "interactive" queues (used by most jobs) is 2h:

<p>
  <table border="1" style="text-align: center;">
  <a name="tbl_maxest">

  <tr>
      <td rowspan="2" 
          style="text-align: center;">
          <font size="+1"><b><i>property</i></b></font></td> 
      <td rowspan="2" >&nbsp;</td>
      <td colspan="4"
         style="text-align: center;">
	 <font size="+1"><b><i>logs</i></b></font>
	 <sup><a href="index.html#note1">1</a></sup>

  <tr>
      <td><b>SDSC-SP2 </b></td>
      <td><b>CTC-SP2  </b></td>
      <td><b>KTH-SP2  </b></td>
      <td><b>SDSC-BLUE</b></td>
      </tr>

  <tr>
      <td><b><i>maxest</i></b></td> 
      <td>&nbsp;</td>
      <td>18h</td>
      <td>18h</td>
      <td><table border="1" style="text-align: center;">
                 <tr><td> 4h<td>weekdays
                 <tr><td>15h<td>weeknights
                 <tr><td>60h<td>weekends</table></td>
      <td><table border="1" style="text-align: center;">
                 <tr><td> 2h<td>express/interactive
                 <tr><td>36h<td>other</table></td>

  <tr>
      <td><b><i>% of jobs</i></b></td> 
      <td>&nbsp;</td>
      <td> 9.7% </td>
      <td>23.8% </td>
      <td>10.1% <sup><a href="index.html#note2">2</a></sup></td>
      <td>27.3% <sup><a href="index.html#note3">3</a> ,
                     <a href="index.html#note4">4</a></sup></td>

  <tr>
      <td><b><i>popularity<br>rank</i></b></td> 
      <td>&nbsp;</td>
      <td>3</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>

  </table>

  <!---- footnotes ---->
  <font size="-1">
  <i>
   1. <a name="note1">using cleaned version of logs<br>
   2. <a name="note2">jobs associated with estimate=4h<br>
   3. <a name="note3">jobs associated with estimate=2h<br>
   4. <a name="note4">including 6% of jobs with estimate=01:59:00<br>
  </i>
  </font>

</p>

</p></li>


<!--------->
<li><p>
Note that even though <tt>maxest</tt> is the upper bound of both
estimates and runtimes, there's usually a very small fraction of the
jobs with runtimes that exceed this limit.
Mostly, the limit is exceeded by less than a minute, and probably
reflects the time it takes to kill the job.
However, on rare occasions, the runtime is much longer (e.g. in
SDSC-SP2 there's a job with runtime > 100h). 
The reason for this phenomenon is unknown, but we speculate that such
jobs were explicitly given special treatment by the system
administrators.
Regardless of the reason, this observation is important when
determining the value of <tt>maxest</tt> that is supplied to the model
(see below).
</p></li>

<!--------->
<li><p>
Modality in general and the vast popularity of <tt>maxest</tt> in
particular are very unfortunate.
Modality is bad because all jobs look the same in the eyes of the
scheduler, which means it has less opportunities to exploit existing
"holes" in the schedule for backfilling.
The popularity of <tt>maxest</tt> is even worse, as jobs that use this
estimate are never backfilled (except from on the expense of the "extra nodes"
[<a
href="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#mualem01"
>mualem01</a>]), they are simply (appearing to be) too long.
</p></li>

<!--------->
<li><p>
Finally, note that the percent of jobs associated with <tt>maxest</tt>
exhibits very high variance.
The good news is that this is where most of the variance is
concentrated, and setting this difference aside, the estimate
distributions of different logs are remarkably similar
[<a
href="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#tsafrir05b"
>tsafrir05b</a>].
Indeed, if the model is supplied with this information (% of jobs
associated with <tt>maxest</tt>), the estimate distribution it
produces is very similar to that of the original.

</p></li>


</ul>
<!----------------------------------------->

<p>
<i>GUIDANCE IN CHOOSING PARAMETERS</i>
</p>

<!----------------------------------------->
<ul>


<!--------->
<li><p>

Due to the reasons mentioned above, the most significant parameters of
the model are (1) the mandatory  <tt>maxest</tt>, and (2) the optional
percent of jobs that are associated with it (can be specified using
the <tt>-b</tt> option).
Let this percent be denoted as <i>P</i>.
</p></li>


<!--------->
<li><p>
<u>How to choose <i>P</i></u>:
The bigger <i>P</i> is, the worse the performance gets.
At the extreme, <i>P=100%</i>, in which case backfilling activity
completely stops (with the exception of "extra nodes" backfilling
[<a
href="http://www.cs.huji.ac.il/labs/parallel/workload/wlbib.html#mualem01"
>mualem01</a>]). 
The scheduling then becomes relatively similar to FCFS.
The default settings of this model sets <i>P</i> to be ~22%.
This is a compromise between the <i>P</i> values listed in the above 
<a href="index.html#tbl_maxest">table</a> and is inclined towards the
bigger values, so that the model will be closer to the worst case.
However, as can be seen, the situation in CTC-SP2 and SDSC-BLUE is
worse.
You can allow the model to use the its compromise value, or you can
make it better (smaller) or worse (bigger), depending on the focus of
your interest.

</p></li>

<!--------->
<li><p>
<u>How to choose <tt>maxest</tt></u>:
When we're appending estimates to a log (that lacks this data) which
was generated by a production system, the best alternative is to find
out what was this site's administrative upper bound on runtimes, and use
this value as <tt>maxest</tt>.
If this information is unavailable, one should examine the runtime
CDF of the log, as plotted in the above figure, and approximate the
upper bound. 
Note that, as explained above, simply using the maximal runtime value
that is found within the log is usually inadequate, as often there's 
a small number of jobs that were given special permission to run
beyond the real administrative upper bound.
Indeed, in the above figures, it is usually quite evident what value
should be used.
<br>
When estimates are appended to a workload that was generated by a
model, best thing to do is to simply use the maximal possible runtime
(if such a model parameter exists), or the biggest runtime value that is
found in the workload, otherwise.


</p></li>

<!--------->
<li><p>
<u>How to choose other user bins</u>:
This is only applicable if there's information available about the
system that generated the workload.
As was shown for SDSC-BLUE, if the administrative runtime upper bounds
of the various queues (if any) are known, it is certainly a good idea
to explicitly specify these values using the <tt>-b</tt> option and
assign to them a significant portion of the jobs. 
Obviously, this should be done while examining the runtime CDF, to
check if your choice is reasonable (e.g. if most jobs run more than an
hour, it is unreasonable to decide that 1h is a very popular estimate
value, as jobs are killed once their estimate is reached).

</p></li>

</ul>


<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="lublin"><h3>Example: adding estimates to the 
   <a href=
   "http://www.cs.huji.ac.il/labs/parallel/workload/models.html#lublin99">
   Lublin model</a>
</h3></a>

<p>
<!----------------------------------------->
<ul>

<!--------->
<li><p>
The first step is to generate the (estimate-less) workload.
We do this by using the original <a 
href="http://www.cs.huji.ac.il/labs/parallel/workload/m_lublin99.c">
C program</a> provided by 
<a href="http://www.cs.huji.ac.il/labs/parallel/workload/models.html#lublin99">
Lublin</a></i>.
By default, Lublin's model generates an SWF-file composed of 1000
jobs only.
We change this to be (say) 50,000 by assigning the new value to the
<tt>SIZE</tt> constant in Lublin's <a 
href="http://www.cs.huji.ac.il/labs/parallel/workload/m_lublin99.c">
program</a> and recompiling it:<pre>
    gcc -lm <a href="lublin/m_lublin99.c">m_lublin99.c</a> -o m_lublin99
</pre>
(last link points to the modified file that generates 50,000 jobs).

</p></li>

<!--------->
<li><p>
We now generate the estimate-less SWF file:<pre>
    ./m_lublin99 > <a href="lublin/m_lublin99.c">lublin.est-less.swf</a>
</pre>

</p></li>

<!--------->
<li><p>
Lublin's model does not define a runtime upper bound, but rather, a
mean runtime (the more jobs there are, the bigger the maximal runtime
gets).
We therefore have no choice but to examine the runtime CDF,
which indicates that while 45h is the maximum, a more natural value
for <tt>maxest</tt> is 10h (e.g. compare with SDSC-SP2's CDF above):

<p>
<img src="lublin/cdf.est-less.gif" border="1">
</p>

</p></li>


<!--------->
<li><p>
There is no additional information available, and so all that's left
is to append the artificial estimates with the chosen
<tt>maxest</tt>:<pre>
     ./est_driver 36000  <a href="lublin/lublin.swf.gz">lublin.est-less.swf</a> > <a href="lublin/lublin.with-est.swf.gz">lublin.swf</a>
</pre>
</p></li>

<!--------->
<li><p>
The resulting estimate distribution looks like so (though obviously
supplying different seeds using the <tt>-s</tt> option will result in
somewhat different distributions):

<p>
<img src="lublin/cdf.gif" border="1">
</p>

</p></li>

</ul>
<!----------------------------------------->



<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="osc"><h3>Example: adding estimates to the 
    <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_osc.html">
    OSC log</a>
</h3></a>

<p>
<!----------------------------------------->
<ul>

<!--------->
<li><p>
We have very little knowledge about the system that generated the <a
href="http://www.cs.huji.ac.il/labs/parallel/workload/l_osc.html">
OSC log</a>, so here too all we have to do is determine
<tt>maxest</tt> by ourselves.
When no information is available, it can be very helpful to inspect
the actual runtime distribution:

<p>
<table border="1">

<tr>
  <td><center><b><i>CDF</i></b></center</td>
  <td><center><b><i>tail runtimes</i></b></center</td>
</tr>
<tr>
   <td><p><img src="osc/cdf.est-less.gif" border="1"></p></td>
   <td><font size="-1">
   <i>[order: top to bottom, left to right]</i><br><br>
   159:48:33,  159:57:35,  160:00:50,  160:02:04,<br>
   159:48:55,  159:57:55,  160:00:51,  160:02:14,<br>
   159:49:06,  160:00:10,  160:01:12,  160:05:58,<br>
   159:49:37,  160:00:21,  160:01:16,  160:09:55,<br>
   159:49:48,  160:00:26,  160:01:22,  163:21:52,<br>
   159:50:15,  160:00:28,  160:01:23,  188:16:04,<br>
   159:53:29,  160:00:34,  160:01:32,  188:16:46,<br>
   159:56:49,  160:00:37,  160:01:35,  306:45:24,<br>
   159:57:09,  160:00:43,  160:01:43,  320:01:51,<br>
   159:57:18,  160:00:47,  160:01:53,  320:02:06
   </pre></font></td>
</tr>
</table>
</p></li>

<!--------->
<li><p>
As can be seen, the maximal runtime is ~320h, but there are only 6
jobs with runtimes that are bigger than 160h by more than a few minutes
and these are scattered across a huge domain.
In contrast, starting from 160h (and downwards), the runtime
distribution becomes continuous, which means choosing <tt>maxest</tt>
to be 160h (= 576,000 seconds) is a reasonable speculation.
</p></li>

<!--------->
<li><p>
Thus, we append estimates to the OSC log as follows:<pre>
     ./est_driver 576000 -i1 <a href="http://www.cs.huji.ac.il/labs/parallel/workload/l_osc_cln.swf.gz">l_osc_cln.swf</a> > <a href="osc/l_osc_cln.with-est.swf.gz">l_osc_cln.with-est.swf</a>
</pre>
which yields the following distribution
<p><img src="osc/cdf.gif" border="1"></p>

</p></li>

</ul>


<!----------------------------------------------------------------------------->
<p>
<img src="line.gif">
<a name="bugs"><h3>Bugs</h3></a>
The <tt>est_driver</tt> utility filters "insane" jobs (with size&lt;1 or
runtime<0) if it operates on an existing SWF file.
Therefore, the number of jobs it prints might be smaller than number
of jobs in the original file.
However, the associated header comment (MaxRecords) remains unchanged
(the new SWF file has an identical header comment to that of the old).

</p>
<!----------------------------------------->

<!---------------------------------------------------------------------------->
<p>
<hr>
</p>

<a href="http://www.cs.huji.ac.il/labs/parallel/workload/index.html">
  Parallel Workloads Archive</a> - 
  <a
  href="http://www.cs.huji.ac.il/labs/parallel/workload/models.html">
  Models</a>
<p>
<hr>
</p>

</body>
</html>

<!--  LocalWords:  IDs LPC est hh cc maxest njobs Dist userbins KTH SP CTC href
 -->
<!--  LocalWords:  LUBLIN'S Lublin Lublin's lublin OSC
 -->
